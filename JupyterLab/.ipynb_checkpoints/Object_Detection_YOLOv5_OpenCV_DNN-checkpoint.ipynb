{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqMVCRZvNCKu"
   },
   "source": [
    "<h1 style=\"font-size40:px;\">Object Detection using YOLOv5</h1>\n",
    "\n",
    "In this module, we will learn how to use YOLOv5 — a state of the art object detector — with OpenCV DNN module.\n",
    "\n",
    "## 1. Why YOLOv5?\n",
    "\n",
    "YOLOv5 was released in 2020 just a month after YOLOv4. Unlike previous versions of YOLO, it is based on PyTorch framework. Although YOLOv5 is not the upgarded version of YOLOv4, it does have several advantages.\n",
    " - Plethora of models\n",
    " - Multi-platform support\n",
    " - Faster than previous versions\n",
    " - Highly active development\n",
    " - PyTorch community is larger than Darknet(YOLOv4 farmework)\n",
    " - Very easy to implement\n",
    "\n",
    "You can choose from five different models, nano, small, medium, large, and extra large. Each having speed-accuracy tradeoff. YOLOv5 officially support [11 platforms](https://github.com/ultralytics/yolov5/releases) so far. Recently, it also extended support to OpenCV. By default, YOLOv5 provides models in PyTorch(.pt) format. However, OpenCV DNN module requires the model to be in ONNX(.onnx) format. So let's begin by converting the PyTorch models to ONNX.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8grNCjv10bb"
   },
   "source": [
    "## 2. Convert PyTorch Models to ONNX\n",
    "\n",
    "We are going to use [git](https://git-scm.com/downloads) and [wget](https://www.gnu.org/software/wget/). Install using the links provided if not already installed.\n",
    "\n",
    "### 2.0 Dependancy Update [22/07/2022]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eiOIkdpZYeK-",
    "outputId": "5ef8b1a3-8738-4bdb-988a-26dc4585ca93",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.11 (from versions: 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.11\u001b[0m\u001b[31m\n",
      "\u001b[0mCollecting opencv-contrib-python==4.6.0.66\n",
      "  Obtaining dependency information for opencv-contrib-python==4.6.0.66 from https://files.pythonhosted.org/packages/34/45/c8bc145b1541d1fbbf25d5494cd76453d9855971cfe571b9ad7e13cdb4c8/opencv_contrib_python-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading opencv_contrib_python-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /home/xabier/anaconda3/lib/python3.11/site-packages (from opencv-contrib-python==4.6.0.66) (1.24.3)\n",
      "Downloading opencv_contrib_python-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: opencv-contrib-python\n",
      "Successfully installed opencv-contrib-python-4.6.0.66\n"
     ]
    }
   ],
   "source": [
    "# Tested with torch==1.11.\n",
    "# Tested with opencv-contrib-python versions [4.5.4.58 to 4.6.0.66].\n",
    "!pip install torch==1.11 torchvision\n",
    "!pip install opencv-contrib-python==4.6.0.66"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BoY6ScQYeK_"
   },
   "source": [
    "### 2.1 Clone UltraLytics YOLOv5 repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SUYnmHWg166Y",
    "outputId": "ec36f4f3-f22f-4866-b915-d8288f0068f5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov5'...\n",
      "remote: Enumerating objects: 16824, done.\u001b[K\n",
      "remote: Total 16824 (delta 0), reused 0 (delta 0), pack-reused 16824\u001b[K\n",
      "Receiving objects: 100% (16824/16824), 15.53 MiB | 3.64 MiB/s, done.\n",
      "Resolving deltas: 100% (11543/11543), done.\n",
      "/home/xabier/Git_repos/MyOpenCVProjects/JupyterLab/yolov5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xabier/.virtualenvs/opencv_environment/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gitpython>=3.1.30 (from -r requirements.txt (line 5))\n",
      "  Obtaining dependency information for gitpython>=3.1.30 from https://files.pythonhosted.org/packages/e9/bd/cc3a402a6439c15c3d4294333e13042b915bbeab54edc457c723931fed3f/GitPython-3.1.43-py3-none-any.whl.metadata\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: matplotlib>=3.3 in /home/xabier/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (3.7.2)\n",
      "Requirement already satisfied: numpy>=1.23.5 in /home/xabier/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (1.24.3)\n",
      "Collecting opencv-python>=4.1.1 (from -r requirements.txt (line 8))\n",
      "  Obtaining dependency information for opencv-python>=4.1.1 from https://files.pythonhosted.org/packages/3f/a4/d2537f47fd7fcfba966bd806e3ec18e7ee1681056d4b0a9c8d983983e4d5/opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting pillow>=10.3.0 (from -r requirements.txt (line 9))\n",
      "  Obtaining dependency information for pillow>=10.3.0 from https://files.pythonhosted.org/packages/ba/e5/8c68ff608a4203085158cff5cc2a3c534ec384536d9438c405ed6370d080/pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata\n",
      "  Downloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: psutil in /home/xabier/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (5.9.0)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /home/xabier/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (6.0)\n",
      "Collecting requests>=2.32.0 (from -r requirements.txt (line 12))\n",
      "  Obtaining dependency information for requests>=2.32.0 from https://files.pythonhosted.org/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl.metadata\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/xabier/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 13)) (1.11.1)\n",
      "Collecting thop>=0.1.1 (from -r requirements.txt (line 14))\n",
      "  Obtaining dependency information for thop>=0.1.1 from https://files.pythonhosted.org/packages/bb/0f/72beeab4ff5221dc47127c80f8834b4bcd0cb36f6ba91c0b1d04a1233403/thop-0.1.1.post2209072238-py3-none-any.whl.metadata\n",
      "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting torch>=1.8.0 (from -r requirements.txt (line 15))\n",
      "  Obtaining dependency information for torch>=1.8.0 from https://files.pythonhosted.org/packages/80/83/9b7681e41e59adb6c2b042f7e8eb716515665a6eed3dda4215c6b3385b90/torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata\n",
      "  Downloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting torchvision>=0.9.0 (from -r requirements.txt (line 16))\n",
      "  Obtaining dependency information for torchvision>=0.9.0 from https://files.pythonhosted.org/packages/42/1d/efde76f826682ebe6ec97c2874f3c7e4833eb84497c521ce6cfac406ef34/torchvision-0.19.0-cp311-cp311-manylinux1_x86_64.whl.metadata\n",
      "  Downloading torchvision-0.19.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /home/xabier/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 17)) (4.65.0)\n",
      "Collecting ultralytics>=8.2.34 (from -r requirements.txt (line 18))\n",
      "  Obtaining dependency information for ultralytics>=8.2.34 from https://files.pythonhosted.org/packages/7b/e0/fec43312f8af41c2bba6aec53c9ff51160e9b10473a603f5388055249b77/ultralytics-8.2.68-py3-none-any.whl.metadata\n",
      "  Downloading ultralytics-8.2.68-py3-none-any.whl.metadata (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.1.4 in /home/xabier/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 27)) (2.0.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /home/xabier/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 28)) (0.12.2)\n",
      "Collecting setuptools>=70.0.0 (from -r requirements.txt (line 42))\n",
      "  Obtaining dependency information for setuptools>=70.0.0 from https://files.pythonhosted.org/packages/51/a0/ee460cc54e68afcf33190d198299c9579a5eafeadef0016ae8563237ccb6/setuptools-71.1.0-py3-none-any.whl.metadata\n",
      "  Using cached setuptools-71.1.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython>=3.1.30->-r requirements.txt (line 5))\n",
      "  Obtaining dependency information for gitdb<5,>=4.0.1 from https://files.pythonhosted.org/packages/fd/5b/8f0c4a5bb9fd491c277c21eff7ccae71b47d43c4446c9d0c6cff2fe8c2c4/gitdb-4.0.11-py3-none-any.whl.metadata\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/xabier/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/xabier/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/xabier/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/xabier/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/xabier/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (23.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /home/xabier/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/xabier/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (2.8.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/xabier/anaconda3/lib/python3.11/site-packages (from requests>=2.32.0->-r requirements.txt (line 12)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/xabier/anaconda3/lib/python3.11/site-packages (from requests>=2.32.0->-r requirements.txt (line 12)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/xabier/anaconda3/lib/python3.11/site-packages (from requests>=2.32.0->-r requirements.txt (line 12)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/xabier/anaconda3/lib/python3.11/site-packages (from requests>=2.32.0->-r requirements.txt (line 12)) (2023.7.22)\n",
      "Requirement already satisfied: filelock in /home/xabier/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.9.0)\n",
      "Collecting typing-extensions>=4.8.0 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
      "  Obtaining dependency information for typing-extensions>=4.8.0 from https://files.pythonhosted.org/packages/26/9f/ad63fc0248c5379346306f8668cda6e2e2e9c95e01216d2b8ffd9ff037d0/typing_extensions-4.12.2-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: sympy in /home/xabier/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/xabier/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/xabier/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/xabier/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (2023.4.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
      "  Obtaining dependency information for nvidia-cuda-nvrtc-cu12==12.1.105 from https://files.pythonhosted.org/packages/b6/9f/c64c03f49d6fbc56196664d05dba14e3a561038a81a638eeb47f4d4cfd48/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
      "  Obtaining dependency information for nvidia-cuda-runtime-cu12==12.1.105 from https://files.pythonhosted.org/packages/eb/d5/c68b1d2cdfcc59e72e8a5949a37ddb22ae6cade80cd4a57a84d4c8b55472/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
      "  Obtaining dependency information for nvidia-cuda-cupti-cu12==12.1.105 from https://files.pythonhosted.org/packages/7e/00/6b218edd739ecfc60524e585ba8e6b00554dd908de2c9c66c1af3e44e18d/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
      "  Obtaining dependency information for nvidia-cudnn-cu12==9.1.0.70 from https://files.pythonhosted.org/packages/9f/fd/713452cd72343f682b1c7b9321e23829f00b842ceaedcda96e742ea0b0b3/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
      "  Obtaining dependency information for nvidia-cublas-cu12==12.1.3.1 from https://files.pythonhosted.org/packages/37/6d/121efd7382d5b0284239f4ab1fc1590d86d34ed4a4a2fdb13b30ca8e5740/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
      "  Obtaining dependency information for nvidia-cufft-cu12==11.0.2.54 from https://files.pythonhosted.org/packages/86/94/eb540db023ce1d162e7bea9f8f5aa781d57c65aed513c33ee9a5123ead4d/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
      "  Obtaining dependency information for nvidia-curand-cu12==10.3.2.106 from https://files.pythonhosted.org/packages/44/31/4890b1c9abc496303412947fc7dcea3d14861720642b49e8ceed89636705/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
      "  Obtaining dependency information for nvidia-cusolver-cu12==11.4.5.107 from https://files.pythonhosted.org/packages/bc/1d/8de1e5c67099015c834315e333911273a8c6aaba78923dd1d1e25fc5f217/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
      "  Obtaining dependency information for nvidia-cusparse-cu12==12.1.0.106 from https://files.pythonhosted.org/packages/65/5b/cfaeebf25cd9fdec14338ccb16f6b2c4c7fa9163aefcf057d86b9cc248bb/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
      "  Obtaining dependency information for nvidia-nccl-cu12==2.20.5 from https://files.pythonhosted.org/packages/4b/2a/0a131f572aa09f741c30ccd45a8e56316e8be8dfc7bc19bf0ab7cfef7b19/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
      "  Obtaining dependency information for nvidia-nvtx-cu12==12.1.105 from https://files.pythonhosted.org/packages/da/d3/8057f0587683ed2fcd4dbfbdfdfa807b9160b809976099d36b8f60d08f03/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch>=1.8.0->-r requirements.txt (line 15))\n",
      "  Obtaining dependency information for triton==3.0.0 from https://files.pythonhosted.org/packages/33/3e/a2f59384587eff6aeb7d37b6780de7fedd2214935e27520430ca9f5b7975/triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->-r requirements.txt (line 15))\n",
      "  Obtaining dependency information for nvidia-nvjitlink-cu12 from https://files.pythonhosted.org/packages/75/bc/e0d0dbb85246a086ab14839979039647bce501d8c661a159b8b019d987b7/nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: py-cpuinfo in /home/xabier/anaconda3/lib/python3.11/site-packages (from ultralytics>=8.2.34->-r requirements.txt (line 18)) (8.0.0)\n",
      "Collecting ultralytics-thop>=2.0.0 (from ultralytics>=8.2.34->-r requirements.txt (line 18))\n",
      "  Obtaining dependency information for ultralytics-thop>=2.0.0 from https://files.pythonhosted.org/packages/89/76/158d54ca16241e9d7007b89966ec952d224132b99f3a6a669fce5517a8a8/ultralytics_thop-2.0.0-py3-none-any.whl.metadata\n",
      "  Downloading ultralytics_thop-2.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/xabier/anaconda3/lib/python3.11/site-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/xabier/anaconda3/lib/python3.11/site-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2023.3)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython>=3.1.30->-r requirements.txt (line 5))\n",
      "  Obtaining dependency information for smmap<6,>=3.0.1 from https://files.pythonhosted.org/packages/a7/a5/10f97f73544edcdef54409f1d839f6049a0d79df68adbc1ceb24d1aaca42/smmap-5.0.1-py3-none-any.whl.metadata\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/xabier/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.3->-r requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/xabier/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.8.0->-r requirements.txt (line 15)) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/xabier/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.8.0->-r requirements.txt (line 15)) (1.3.0)\n",
      "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
      "Downloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl (797.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.3/797.3 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.6/410.6 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:45\u001b[0m"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5\n",
    "%cd yolov5\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_FJMobz-91-"
   },
   "source": [
    "### 2.2 Download YOLOv5 PyTorch Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jySqouqU2Y3N",
    "outputId": "96ad05cb-e682-4fff-d0fe-48d588fde22e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/yolov5/models\n",
      "2024-05-10 09:55:42 URL:https://objects.githubusercontent.com/github-production-release-asset-2e65be/264818686/e3cd2b74-b7e5-491d-901a-58234d5f948f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240510%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240510T095541Z&X-Amz-Expires=300&X-Amz-Signature=4a1502545521854a6ff761f8c74d44640f8831d437c25608c064003970b3caf2&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=264818686&response-content-disposition=attachment%3B%20filename%3Dyolov5n.pt&response-content-type=application%2Foctet-stream [4062133/4062133] -> \"yolov5n.pt\" [1]\n",
      "2024-05-10 09:55:42 URL:https://objects.githubusercontent.com/github-production-release-asset-2e65be/264818686/76813c2d-b52b-47af-95fb-e92c1b0b2783?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240510%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240510T095542Z&X-Amz-Expires=300&X-Amz-Signature=30ab7ebdd9ac3fb5a4ef3b93f0ac30dc53527ca45dbe0c77378ed3af650659c4&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=264818686&response-content-disposition=attachment%3B%20filename%3Dyolov5s.pt&response-content-type=application%2Foctet-stream [14808437/14808437] -> \"yolov5s.pt\" [1]\n",
      "2024-05-10 09:55:43 URL:https://objects.githubusercontent.com/github-production-release-asset-2e65be/264818686/77bb6f54-77c5-4161-b921-b225b7bb730e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240510%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240510T095543Z&X-Amz-Expires=300&X-Amz-Signature=218d38db6d954a8f10d8d1f16dbddbb0a198eb6157896a11d5581e72729f93d7&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=264818686&response-content-disposition=attachment%3B%20filename%3Dyolov5m.pt&response-content-type=application%2Foctet-stream [42806829/42806829] -> \"yolov5m.pt\" [1]\n",
      "2024-05-10 09:55:45 URL:https://objects.githubusercontent.com/github-production-release-asset-2e65be/264818686/e8786ec9-9cbf-41e1-b75d-d4b4649df8c5?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240510%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240510T095543Z&X-Amz-Expires=300&X-Amz-Signature=a360f57a76379b5b66ff99605977b2cbe5981837fa3437a2dfac07196436c5f0&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=264818686&response-content-disposition=attachment%3B%20filename%3Dyolov5l.pt&response-content-type=application%2Foctet-stream [93622629/93622629] -> \"yolov5l.pt\" [1]\n",
      "2024-05-10 09:55:49 URL:https://objects.githubusercontent.com/github-production-release-asset-2e65be/264818686/a7014a41-f517-4ef0-ad4e-6ccab3ad2a94?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240510%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240510T095545Z&X-Amz-Expires=300&X-Amz-Signature=e333bdcf97a6f7a3907452209f310deaf7fab7764430f2a8007465868e38398e&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=264818686&response-content-disposition=attachment%3B%20filename%3Dyolov5x.pt&response-content-type=application%2Foctet-stream [174114333/174114333] -> \"yolov5x.pt\" [1]\n",
      "/content/yolov5\n"
     ]
    }
   ],
   "source": [
    "%cd models\n",
    "!wget https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5n.pt -nv\n",
    "!wget https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5s.pt -nv\n",
    "!wget https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5m.pt -nv\n",
    "!wget https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5l.pt -nv\n",
    "!wget https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5x.pt -nv\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkQ-TSVy_EoS"
   },
   "source": [
    "### 2.3 Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o1zNjFFd2zOl",
    "outputId": "ba67616b-ea05-489c-e50e-673369794777"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mexport: \u001b[0mdata=data/coco128.yaml, weights=['models/yolov5n.pt'], imgsz=[640, 640], batch_size=1, device=cpu, half=False, inplace=False, keras=False, optimize=False, int8=False, per_tensor=False, dynamic=False, simplify=False, opset=12, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['onnx']\n",
      "YOLOv5 🚀 v7.0-307-g920c721e Python-3.10.12 torch-1.11.0+cu102 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from models/yolov5n.pt with output shape (1, 25200, 85) (3.9 MB)\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['onnx>=1.12.0'] not found, attempting AutoUpdate...\n",
      "Collecting onnx>=1.12.0\n",
      "  Downloading onnx-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.9/15.9 MB 70.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.12.0) (1.25.2)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.12.0) (3.20.3)\n",
      "Installing collected packages: onnx\n",
      "Successfully installed onnx-1.16.0\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 16.0s, installed 1 package: ['onnx>=1.12.0']\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.16.0...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 18.9s, saved as models/yolov5n.onnx (7.5 MB)\n",
      "\n",
      "Export complete (19.3s)\n",
      "Results saved to \u001b[1m/content/yolov5/models\u001b[0m\n",
      "Detect:          python detect.py --weights models/yolov5n.onnx \n",
      "Validate:        python val.py --weights models/yolov5n.onnx \n",
      "PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'models/yolov5n.onnx')  \n",
      "Visualize:       https://netron.app\n",
      "\u001b[34m\u001b[1mexport: \u001b[0mdata=data/coco128.yaml, weights=['models/yolov5s.pt'], imgsz=[640, 640], batch_size=1, device=cpu, half=False, inplace=False, keras=False, optimize=False, int8=False, per_tensor=False, dynamic=False, simplify=False, opset=12, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['onnx']\n",
      "YOLOv5 🚀 v7.0-307-g920c721e Python-3.10.12 torch-1.11.0+cu102 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from models/yolov5s.pt with output shape (1, 25200, 85) (14.1 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.16.0...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 2.5s, saved as models/yolov5s.onnx (28.0 MB)\n",
      "\n",
      "Export complete (3.4s)\n",
      "Results saved to \u001b[1m/content/yolov5/models\u001b[0m\n",
      "Detect:          python detect.py --weights models/yolov5s.onnx \n",
      "Validate:        python val.py --weights models/yolov5s.onnx \n",
      "PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'models/yolov5s.onnx')  \n",
      "Visualize:       https://netron.app\n",
      "\u001b[34m\u001b[1mexport: \u001b[0mdata=data/coco128.yaml, weights=['models/yolov5m.pt'], imgsz=[640, 640], batch_size=1, device=cpu, half=False, inplace=False, keras=False, optimize=False, int8=False, per_tensor=False, dynamic=False, simplify=False, opset=12, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['onnx']\n",
      "YOLOv5 🚀 v7.0-307-g920c721e Python-3.10.12 torch-1.11.0+cu102 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from models/yolov5m.pt with output shape (1, 25200, 85) (40.8 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.16.0...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 4.9s, saved as models/yolov5m.onnx (81.2 MB)\n",
      "\n",
      "Export complete (7.9s)\n",
      "Results saved to \u001b[1m/content/yolov5/models\u001b[0m\n",
      "Detect:          python detect.py --weights models/yolov5m.onnx \n",
      "Validate:        python val.py --weights models/yolov5m.onnx \n",
      "PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'models/yolov5m.onnx')  \n",
      "Visualize:       https://netron.app\n",
      "\u001b[34m\u001b[1mexport: \u001b[0mdata=data/coco128.yaml, weights=['models/yolov5l.pt'], imgsz=[640, 640], batch_size=1, device=cpu, half=False, inplace=False, keras=False, optimize=False, int8=False, per_tensor=False, dynamic=False, simplify=False, opset=12, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['onnx']\n",
      "YOLOv5 🚀 v7.0-307-g920c721e Python-3.10.12 torch-1.11.0+cu102 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5l summary: 367 layers, 46533693 parameters, 0 gradients\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from models/yolov5l.pt with output shape (1, 25200, 85) (89.3 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.16.0...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 7.6s, saved as models/yolov5l.onnx (177.9 MB)\n",
      "\n",
      "Export complete (13.5s)\n",
      "Results saved to \u001b[1m/content/yolov5/models\u001b[0m\n",
      "Detect:          python detect.py --weights models/yolov5l.onnx \n",
      "Validate:        python val.py --weights models/yolov5l.onnx \n",
      "PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'models/yolov5l.onnx')  \n",
      "Visualize:       https://netron.app\n",
      "\u001b[34m\u001b[1mexport: \u001b[0mdata=data/coco128.yaml, weights=['models/yolov5x.pt'], imgsz=[640, 640], batch_size=1, device=cpu, half=False, inplace=False, keras=False, optimize=False, int8=False, per_tensor=False, dynamic=False, simplify=False, opset=12, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['onnx']\n",
      "YOLOv5 🚀 v7.0-307-g920c721e Python-3.10.12 torch-1.11.0+cu102 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5x summary: 444 layers, 86705005 parameters, 0 gradients\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from models/yolov5x.pt with output shape (1, 25200, 85) (166.0 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.16.0...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 16.6s, saved as models/yolov5x.onnx (331.2 MB)\n",
      "\n",
      "Export complete (26.7s)\n",
      "Results saved to \u001b[1m/content/yolov5/models\u001b[0m\n",
      "Detect:          python detect.py --weights models/yolov5x.onnx \n",
      "Validate:        python val.py --weights models/yolov5x.onnx \n",
      "PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'models/yolov5x.onnx')  \n",
      "Visualize:       https://netron.app\n"
     ]
    }
   ],
   "source": [
    "# The default input size is 640x640.\n",
    "!python export.py --weights models/yolov5n.pt --include onnx --opset 12\n",
    "!python export.py --weights models/yolov5s.pt --include onnx --opset 12\n",
    "!python export.py --weights models/yolov5m.pt --include onnx --opset 12\n",
    "!python export.py --weights models/yolov5l.pt --include onnx --opset 12\n",
    "!python export.py --weights models/yolov5x.pt --include onnx --opset 12\n",
    "\n",
    "# Add the flag --imsz to export for custom input size.\n",
    "# !python export.py --weights models/yolov5n.pt --include onnx --imsz 320 320\n",
    "\n",
    "# Add the flag --dyanamic for dynamic input size. Compatible with ONNX runtime.\n",
    "# !python export.py --weights models/yolov5n.pt --include onnx --dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTjH17cn7GDm"
   },
   "source": [
    "## 3. Object Detection with YOLOv5 and OpenCV DNN module\n",
    "Let us now see how to use YOLOv5 in OpenCV to perform object detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZP6u98_vAdiw"
   },
   "source": [
    "### 3.1 Why use YOLOv5 with OpenCV?\n",
    "1. **Easy integration with an OpenCV application:** If your application already uses OpenCV and you simply want to use YOLOv5, you don't have to worry about building it from source.\n",
    "2. **Faster:** OpenCV is Optimized for Intel CPUs. We can see significant performance leap using OpenCV DNN module.\n",
    "3. **C++ support:** As mentioned above, YOLOv5 is Python based. Although it  is a popular language, industry grade solutions are still being built in C++ due to its efficiency. We can not implement YOLOv5 in C++ directly but using OpenCV, it's possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMRGfqXbDER5"
   },
   "source": [
    "### 3.2 Download Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8FRAyfZrNCKy",
    "outputId": "0ab5caf1-7533-4428-9101-c9f237512661",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Downloading Resources...\")\n",
    "!wget https://www.dropbox.com/s/hhx3pbacs7rixid/street.jpg?dl=1 -O street.jpg -nv\n",
    "!wget https://www.dropbox.com/s/wb5nkwuml526bqa/coco.names?dl=1 -O coco.names -nv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVmCht7kDP8l"
   },
   "source": [
    "### 3.3 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijXKmlmVNCKz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "import requests\n",
    "from os import path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VQ6F7qAuNCK0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (15.0,15.0)\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXsyNc4cNCK3"
   },
   "source": [
    "### 3.4: Initialize the parameters\n",
    "The YOLOv5 algorithm generates bounding boxes as the predicted detection outputs. Every predicted box is associated with a confidence score. In the first stage, all the boxes below the confidence threshold parameter are ignored for further processing.\n",
    "\n",
    "Every detection also has a class scores associated with it. Class scores are the probability of the detection being the object from the dataset it is trained on. YOLOv5 is trained on COCO dataset 2017 which has 80 classes.\n",
    "\n",
    "Each object can have multiple bounding boxes. Since we need only one, rest of the boxes are passed through non-maximum suppression. **Non-maximum suppression** is controlled by a parameter **NMS_THRESHOLD**. You can try to change these values and see how the number of output predicted boxes changes.\n",
    "\n",
    "Next, the default values for the input width (**INPUT_WIDTH**) and height (**INPUT_HEIGHT**) for the network’s input image are set. We set each of them to 640 (default). You can use smaller size(Multiple of 32) to increase the speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j-zZyFamNCK4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Constants.\n",
    "INPUT_WIDTH = 640             # Width of network's input image, larger is slower but more accurate\n",
    "INPUT_HEIGHT = 640            # Height of network's input image, larger is slower but more accurate\n",
    "SCORE_THRESHOLD = 0.5         # Class score threshold, accepts only if score is above the threshold.\n",
    "NMS_THRESHOLD = 0.45          # Non-maximum suppression threshold, higher values result in duplicate boxes per object\n",
    "CONFIDENCE_THRESHOLD = 0.45   # Confidence threshold, high values filter out low confidence detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-eiIY1xoZla3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Text parameters.\n",
    "FONT_FACE = cv2.FONT_HERSHEY_SIMPLEX\n",
    "FONT_SCALE = 2.5\n",
    "THICKNESS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7VysjPdVZnDJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Colors\n",
    "BLACK  = (0,0,0)\n",
    "BLUE   = (255,178,50)\n",
    "YELLOW = (0,255,255)\n",
    "RED = (0,0,255)\n",
    "WHITE = (255,255,255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJLOKpdyNCK4"
   },
   "source": [
    "### 3.5 Function to pre-process the image\n",
    "\n",
    "The function pre–process takes the image and the network as arguments. At first, the image is converted to a blob. Then it is set as input to the network. The function `getUnconnectedOutLayerNames()` provides the names of the output layers. It has features of all the layers, through which the image is forward propagated to acquire the detections. After processing, it returns the detection results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UBIjrNYCNCK4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pre_process(input_image, net):\n",
    "    # Create a 4D blob from a frame.\n",
    "    blob = cv2.dnn.blobFromImage(input_image, 1/255, (INPUT_WIDTH, INPUT_HEIGHT), [0,0,0], 1, crop=False)\n",
    "\n",
    "    # Sets the input to the network.\n",
    "    net.setInput(blob)\n",
    "\n",
    "    # Runs the forward pass to get output of the output layers.\n",
    "    output_layers = net.getUnconnectedOutLayersNames()\n",
    "    outputs = net.forward(output_layers)\n",
    "    # print(outputs[0].shape)\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rj7hyJIfNCK6"
   },
   "source": [
    "### 3.6 Post-processing the network’s output\n",
    "\n",
    "The network outputs bounding boxes are each represented by 5 elements + a vector of 80 classes. The first 4 elements represent the **center_x**, **center_y**, **width** and **height**. The fifth element represents the confidence that the bounding box encloses an object.\n",
    "\n",
    "The rest of the elements are the confidence associated with each class (i.e. object type). The box itself is assigned to the class corresponding to the highest score. The highest score for a box is also called its **confidence**. If the confidence of a box is less than the given threshold, the bounding box is dropped and not considered for further processing.\n",
    "\n",
    "The boxes with their confidence equal to or greater than the confidence threshold are then subjected to Non Maximum Suppression. This would reduce the number of overlapping boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYSM1rW-kv3n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def post_process(input_image, outputs):\n",
    "    # Lists to hold respective values while unwrapping.\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    # Rows.\n",
    "    rows = outputs[0].shape[1]\n",
    "\n",
    "    image_height, image_width = input_image.shape[:2]\n",
    "\n",
    "    # Resizing factor.\n",
    "    x_factor = image_width / INPUT_WIDTH\n",
    "    y_factor =  image_height / INPUT_HEIGHT\n",
    "\n",
    "    # Iterate through 25200 detections.\n",
    "    for r in range(rows):\n",
    "        row = outputs[0][0][r]\n",
    "        confidence = row[4]\n",
    "\n",
    "        # Discard bad detections and continue.\n",
    "        if confidence >= CONFIDENCE_THRESHOLD:\n",
    "            classes_scores = row[5:]\n",
    "\n",
    "            # Get the index of max class score.\n",
    "            class_id = np.argmax(classes_scores)\n",
    "\n",
    "            #  Continue if the class score is above threshold.\n",
    "            if (classes_scores[class_id] > SCORE_THRESHOLD):\n",
    "                confidences.append(confidence)\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "                cx, cy, w, h = row[0], row[1], row[2], row[3]\n",
    "\n",
    "                left = int((cx - w/2) * x_factor)\n",
    "                top = int((cy - h/2) * y_factor)\n",
    "                width = int(w * x_factor)\n",
    "                height = int(h * y_factor)\n",
    "\n",
    "                box = np.array([left, top, width, height])\n",
    "                boxes.append(box)\n",
    "\n",
    "    # Perform non maximum suppression to eliminate redundant overlapping boxes with\n",
    "    # lower confidences.\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, CONFIDENCE_THRESHOLD, NMS_THRESHOLD)\n",
    "    for i in indices:\n",
    "        box = boxes[i]\n",
    "        left = box[0]\n",
    "        top = box[1]\n",
    "        width = box[2]\n",
    "        height = box[3]\n",
    "        cv2.rectangle(input_image, (left, top), (left + width, top + height), BLUE, 4*THICKNESS)\n",
    "        label = \"{}:{:.2f}\".format(classes[class_ids[i]], confidences[i])\n",
    "        draw_label(input_image, label, left, top)\n",
    "\n",
    "    return input_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHcRq1n2NCK6"
   },
   "source": [
    "The Non Maximum Suppression is controlled by the nmsThreshold parameter. If nmsThreshold is set too low, e.g. 0.1, we might not detect overlapping objects of same or different classes. But if it is set too high e.g. 1, then we get multiple boxes for the same object. So we used an intermediate value of 0.4 in our code above. The gif below shows the effect of varying the NMS threshold.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<a href=\"https://courses.opencv.org/asset-v1:OpenCV+101+Beginners+type@asset+block@nms-car.gif\"><img src=\"https://courses.opencv.org/asset-v1:OpenCV+101+Beginners+type@asset+block@nms-car.gif\"/> </a>\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "<center>non maximum suppression threshold object detection</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_tyYch_lO9H"
   },
   "source": [
    "### 3.7 Function to draw labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZt995NUlPTP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def draw_label(input_image, label, left, top):\n",
    "    \"\"\"Draw text onto image at location.\"\"\"\n",
    "\n",
    "    # Get text size.\n",
    "    text_size = cv2.getTextSize(label, FONT_FACE, FONT_SCALE, THICKNESS)\n",
    "    dim, baseline = text_size[0], text_size[1]\n",
    "    # Use text size to create a BLACK rectangle.\n",
    "    cv2.rectangle(input_image, (left, top), (left + dim[0], top + dim[1] + baseline), BLACK, cv2.FILLED);\n",
    "    # Display text inside the rectangle.\n",
    "    cv2.putText(input_image, label, (left, top + dim[1]), FONT_FACE, FONT_SCALE, YELLOW, THICKNESS, cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AS6Mzjxctdia"
   },
   "source": [
    "### 3.8 Function to put efficiency information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HirE0vOQtisX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def put_efficiency(input_img, net):\n",
    "  t, _ = net.getPerfProfile()\n",
    "  label = 'Inference time: %.2f ms' % (t * 1000.0 / cv2.getTickFrequency())\n",
    "  print(label)\n",
    "  cv2.putText(input_img, label, (20, 80), FONT_FACE, FONT_SCALE, RED, THICKNESS, cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXcuLZGHfIJz"
   },
   "source": [
    "### 3.9 Load image and class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 729
    },
    "id": "mQM7ScDofHka",
    "outputId": "64bfe61a-7847-47c7-e65e-a726a99fabf5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "frame = cv2.imread('street.jpg')\n",
    "\n",
    "classesFile = \"coco.names\"\n",
    "classes = None\n",
    "with open(classesFile, 'rt') as f:\n",
    "  classes = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    "plt.imshow(frame[...,::-1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U12PFQ-aNCK7"
   },
   "source": [
    "# 4. RESULTS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsnuWo2jSt6o"
   },
   "source": [
    "### 4.1 YOLOv5 Nano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 747
    },
    "id": "AD5C2njRS370",
    "outputId": "3b79e036-157d-4196-fe25-39da720ef1c2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the network.\n",
    "modelWeights = \"models/yolov5n.onnx\"\n",
    "net = cv2.dnn.readNet(modelWeights)\n",
    "\n",
    "# Process image.\n",
    "detections = pre_process(frame, net)\n",
    "img = post_process(frame.copy(), detections)\n",
    "\n",
    "# Put efficiency information.\n",
    "put_efficiency(img, net)\n",
    "\n",
    "plt.imshow(img[...,::-1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUL-gw3EGVnW"
   },
   "source": [
    "### 4.2 YOLOv5 Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 747
    },
    "id": "9dScJsl0iLe0",
    "outputId": "99f5fa86-aea3-46db-a32b-fc3382d0197f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the network.\n",
    "modelWeights = \"models/yolov5s.onnx\"\n",
    "net = cv2.dnn.readNet(modelWeights)\n",
    "\n",
    "# Process image.\n",
    "detections = pre_process(frame, net)\n",
    "img = post_process(frame.copy(), detections)\n",
    "\n",
    "# Put efficiency information.\n",
    "put_efficiency(img, net)\n",
    "\n",
    "plt.imshow(img[...,::-1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQdAogEJGbpW"
   },
   "source": [
    "### 4.3 YOLOv5 Medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 747
    },
    "id": "_80tp8HlFV2J",
    "outputId": "be0ca32a-6db7-4346-b7dd-f5792f318268",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the network.\n",
    "modelWeights = \"models/yolov5m.onnx\"\n",
    "net = cv2.dnn.readNet(modelWeights)\n",
    "\n",
    "# Process image.\n",
    "detections = pre_process(frame, net)\n",
    "img = post_process(frame.copy(), detections)\n",
    "\n",
    "# Put efficiency information.\n",
    "put_efficiency(img, net)\n",
    "\n",
    "plt.imshow(img[...,::-1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uu8BGqD9Ia9u"
   },
   "source": [
    "### 4.4 YOLOv5 Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 747
    },
    "id": "wJ2x8T60FYRp",
    "outputId": "308e4653-c77b-4793-cbdc-13bcbfc83ca6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the network.\n",
    "modelWeights = \"models/yolov5l.onnx\"\n",
    "net = cv2.dnn.readNet(modelWeights)\n",
    "\n",
    "# Process image.\n",
    "detections = pre_process(frame, net)\n",
    "img = post_process(frame.copy(), detections)\n",
    "\n",
    "# Put efficiency information.\n",
    "put_efficiency(img, net)\n",
    "\n",
    "plt.imshow(img[...,::-1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQnDuL-0IiOv"
   },
   "source": [
    "### 4.5 YOLOv5 Extra Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 747
    },
    "id": "z-GVMsoDFZ4A",
    "outputId": "f258ca10-8296-4385-8d5a-6a27a59cf282",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the network.\n",
    "modelWeights = \"models/yolov5x.onnx\"\n",
    "net = cv2.dnn.readNet(modelWeights)\n",
    "\n",
    "# Process image.\n",
    "detections = pre_process(frame, net)\n",
    "img = post_process(frame.copy(), detections)\n",
    "\n",
    "# Put efficiency information.\n",
    "put_efficiency(img, net)\n",
    "\n",
    "plt.imshow(img[...,::-1]);"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "opencv_environment",
   "language": "python",
   "name": "opencv_environment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
